wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: manurslf (manurslf301) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_110934-0mutbe73
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 2
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/0mutbe73
Operation on cuda
Using midjourney DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 2
Training...
Loss epoch 0 -> 0.96158
Loss epoch 1 -> 0.23592
Loss epoch 2 -> 0.10616
Loss epoch 3 -> 0.04668
Loss epoch 4 -> 0.04614
Loss epoch 5 -> 0.01885
Loss epoch 6 -> 0.02675
Loss epoch 7 -> 0.03698
Loss epoch 8 -> 0.04227
Loss epoch 9 -> 0.04788
Loss epoch 10 -> 0.01522
Loss epoch 11 -> 0.01236
Loss epoch 12 -> 0.02231
Loss epoch 13 -> 0.01788
Loss epoch 14 -> 0.00888
Loss epoch 15 -> 0.01202
Loss epoch 16 -> 0.01589
Loss epoch 17 -> 0.01504
Loss epoch 18 -> 0.00667
Loss epoch 19 -> 0.01308
Loss epoch 20 -> 0.01847
Loss epoch 21 -> 0.01733
Loss epoch 22 -> 0.01967
Loss epoch 23 -> 0.02575
Loss epoch 24 -> 0.06466
Loss epoch 25 -> 0.02421
Loss epoch 26 -> 0.00840
Loss epoch 27 -> 0.00472
Loss epoch 28 -> 0.01205
Loss epoch 29 -> 0.01433
Loss epoch 30 -> 0.01326
Loss epoch 31 -> 0.00287
Loss epoch 32 -> 0.00018
Loss epoch 33 -> 0.00004
Loss epoch 34 -> 0.00003
Loss epoch 35 -> 0.00002
Loss epoch 36 -> 0.00002
Loss epoch 37 -> 0.00002
Loss epoch 38 -> 0.00002
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–„â–ƒâ–…â–…â–†â–…â–…â–†â–‡â–†â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–„â–ˆâ–‡â–†â–ˆâ–ˆâ–†â–ˆâ–†â–‡â–‡â–†â–†â–†â–†â–†â–†
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.897
wandb:    Train/Loss 1e-05
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 2 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/0mutbe73
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_110934-0mutbe73/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_115135-5ukenb5o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 3
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/5ukenb5o
Loss epoch 39 -> 0.00001
end...
Accuracy : 0.897
Classification report:
              precision    recall  f1-score   support

          ia       0.89      0.90      0.90       500
      nature       0.90      0.89      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 3
Training...
Loss epoch 0 -> 1.04582
Loss epoch 1 -> 0.33250
Loss epoch 2 -> 0.14318
Loss epoch 3 -> 0.05953
Loss epoch 4 -> 0.05804
Loss epoch 5 -> 0.04203
Loss epoch 6 -> 0.03990
Loss epoch 7 -> 0.03370
Loss epoch 8 -> 0.03253
Loss epoch 9 -> 0.02455
Loss epoch 10 -> 0.02528
Loss epoch 11 -> 0.02589
Loss epoch 12 -> 0.02131
Loss epoch 13 -> 0.03434
Loss epoch 14 -> 0.02074
Loss epoch 15 -> 0.02288
Loss epoch 16 -> 0.02879
Loss epoch 17 -> 0.01776
Loss epoch 18 -> 0.02715
Loss epoch 19 -> 0.03305
Loss epoch 20 -> 0.02896
Loss epoch 21 -> 0.02487
Loss epoch 22 -> 0.01830
Loss epoch 23 -> 0.10038
Loss epoch 24 -> 0.02952
Loss epoch 25 -> 0.01498
Loss epoch 26 -> 0.00363
Loss epoch 27 -> 0.00132
Loss epoch 28 -> 0.00507
Loss epoch 29 -> 0.01414
Loss epoch 30 -> 0.01419
Loss epoch 31 -> 0.00935
Loss epoch 32 -> 0.00463
Loss epoch 33 -> 0.00292
Loss epoch 34 -> 0.00146
Loss epoch 35 -> 0.02307
Loss epoch 36 -> 0.02354
Loss epoch 37 -> 0.01458
Loss epoch 38 -> 0.00689
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–…â–…â–„â–†â–‡â–…â–‡â–‡â–‡â–…â–‡â–ˆâ–†â–‡â–†â–†â–†â–†â–†â–†â–…â–†â–†â–†â–…â–†â–†â–…â–†â–†â–†â–†â–‡â–†â–†â–…â–†â–†â–‡
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.901
wandb:    Train/Loss 0.00209
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 3 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/5ukenb5o
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_115135-5ukenb5o/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_123343-n6ee9zbh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 4
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/n6ee9zbh
Loss epoch 39 -> 0.00209
end...
Accuracy : 0.901
Classification report:
              precision    recall  f1-score   support

          ia       0.88      0.92      0.90       500
      nature       0.92      0.88      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 4
Training...
Loss epoch 0 -> 1.30433
Loss epoch 1 -> 0.52257
Loss epoch 2 -> 0.25078
Loss epoch 3 -> 0.10711
Loss epoch 4 -> 0.07309
Loss epoch 5 -> 0.05414
Loss epoch 6 -> 0.04185
Loss epoch 7 -> 0.03780
Loss epoch 8 -> 0.02001
Loss epoch 9 -> 0.02270
Loss epoch 10 -> 0.01541
Loss epoch 11 -> 0.02881
Loss epoch 12 -> 0.01152
Loss epoch 13 -> 0.01626
Loss epoch 14 -> 0.02927
Loss epoch 15 -> 0.02132
Loss epoch 16 -> 0.01056
Loss epoch 17 -> 0.02183
Loss epoch 18 -> 0.01641
Loss epoch 19 -> 0.01945
Loss epoch 20 -> 0.01826
Loss epoch 21 -> 0.02049
Loss epoch 22 -> 0.01638
Loss epoch 23 -> 0.01132
Loss epoch 24 -> 0.04134
Loss epoch 25 -> 0.03838
Loss epoch 26 -> 0.03232
Loss epoch 27 -> 0.02058
Loss epoch 28 -> 0.00333
Loss epoch 29 -> 0.00480
Loss epoch 30 -> 0.00632
Loss epoch 31 -> 0.00394
Loss epoch 32 -> 0.00041
Loss epoch 33 -> 0.00005
Loss epoch 34 -> 0.00002
Loss epoch 35 -> 0.00002
Loss epoch 36 -> 0.00002
Loss epoch 37 -> 0.00001
Loss epoch 38 -> 0.00001
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    Train/Loss â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.883
wandb:    Train/Loss 1e-05
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 4 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/n6ee9zbh
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_123343-n6ee9zbh/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_131552-kbxwcjxk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 5
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/kbxwcjxk
Loss epoch 39 -> 0.00001
end...
Accuracy : 0.883
Classification report:
              precision    recall  f1-score   support

          ia       0.87      0.90      0.88       500
      nature       0.89      0.87      0.88       500

    accuracy                           0.88      1000
   macro avg       0.88      0.88      0.88      1000
weighted avg       0.88      0.88      0.88      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 5
Training...
Loss epoch 0 -> 1.02193
Loss epoch 1 -> 0.25708
Loss epoch 2 -> 0.09956
Loss epoch 3 -> 0.05732
Loss epoch 4 -> 0.04232
Loss epoch 5 -> 0.02939
Loss epoch 6 -> 0.04533
Loss epoch 7 -> 0.03875
Loss epoch 8 -> 0.02388
Loss epoch 9 -> 0.01893
Loss epoch 10 -> 0.03714
Loss epoch 11 -> 0.02283
Loss epoch 12 -> 0.01550
Loss epoch 13 -> 0.01865
Loss epoch 14 -> 0.03189
Loss epoch 15 -> 0.01714
Loss epoch 16 -> 0.00648
Loss epoch 17 -> 0.00538
Loss epoch 18 -> 0.01060
Loss epoch 19 -> 0.00650
Loss epoch 20 -> 0.02278
Loss epoch 21 -> 0.03518
Loss epoch 22 -> 0.02238
Loss epoch 23 -> 0.02227
Loss epoch 24 -> 0.02508
Loss epoch 25 -> 0.01872
Loss epoch 26 -> 0.00554
Loss epoch 27 -> 0.00989
Loss epoch 28 -> 0.00734
Loss epoch 29 -> 0.01685
Loss epoch 30 -> 0.01296
Loss epoch 31 -> 0.02350
Loss epoch 32 -> 0.01353
Loss epoch 33 -> 0.03823
Loss epoch 34 -> 0.01505
Loss epoch 35 -> 0.00251
Loss epoch 36 -> 0.00021
Loss epoch 37 -> 0.00005
Loss epoch 38 -> 0.00003
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–†â–…â–„â–ˆâ–‡â–ˆâ–ˆâ–„â–„â–†â–†â–…â–ƒâ–‡â–†â–†â–†â–…â–…â–„â–„â–ƒâ–‡â–‡â–†â–‡â–ˆâ–†â–†â–†â–ˆâ–‡â–„â–‡â–‡â–‡â–†â–‡â–‡
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.889
wandb:    Train/Loss 2e-05
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 5 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/kbxwcjxk
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_131552-kbxwcjxk/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_135806-w8bylji6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 6
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/w8bylji6
Loss epoch 39 -> 0.00002
end...
Accuracy : 0.889
Classification report:
              precision    recall  f1-score   support

          ia       0.87      0.91      0.89       500
      nature       0.90      0.87      0.89       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 6
Training...
Loss epoch 0 -> 1.17594
Loss epoch 1 -> 0.27433
Loss epoch 2 -> 0.11223
Loss epoch 3 -> 0.04464
Loss epoch 4 -> 0.05550
Loss epoch 5 -> 0.03649
Loss epoch 6 -> 0.03511
Loss epoch 7 -> 0.01425
Loss epoch 8 -> 0.03815
Loss epoch 9 -> 0.03520
Loss epoch 10 -> 0.08081
Loss epoch 11 -> 0.07288
Loss epoch 12 -> 0.02860
Loss epoch 13 -> 0.03118
Loss epoch 14 -> 0.01606
Loss epoch 15 -> 0.02538
Loss epoch 16 -> 0.01886
Loss epoch 17 -> 0.01473
Loss epoch 18 -> 0.00433
Loss epoch 19 -> 0.00590
Loss epoch 20 -> 0.02950
Loss epoch 21 -> 0.01110
Loss epoch 22 -> 0.00491
Loss epoch 23 -> 0.00165
Loss epoch 24 -> 0.00055
Loss epoch 25 -> 0.00458
Loss epoch 26 -> 0.00660
Loss epoch 27 -> 0.01951
Loss epoch 28 -> 0.03320
Loss epoch 29 -> 0.01922
Loss epoch 30 -> 0.01653
Loss epoch 31 -> 0.03420
Loss epoch 32 -> 0.06129
Loss epoch 33 -> 0.03369
Loss epoch 34 -> 0.01628
Loss epoch 35 -> 0.01145
Loss epoch 36 -> 0.00728
Loss epoch 37 -> 0.00619
Loss epoch 38 -> 0.01684
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–†â–„â–‡â–ˆâ–†â–†â–‡â–ˆâ–‡â–‡â–†â–†â–‡â–ˆâ–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.895
wandb:    Train/Loss 0.01294
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 6 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/w8bylji6
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_135806-w8bylji6/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_144024-yrpw8vli
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 7
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/yrpw8vli
Loss epoch 39 -> 0.01294
end...
Accuracy : 0.895
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.89      0.89       500
      nature       0.89      0.90      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.89      1000
weighted avg       0.90      0.90      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 7
Training...
Loss epoch 0 -> 1.04189
Loss epoch 1 -> 0.25050
Loss epoch 2 -> 0.09704
Loss epoch 3 -> 0.04736
Loss epoch 4 -> 0.05372
Loss epoch 5 -> 0.04274
Loss epoch 6 -> 0.02612
Loss epoch 7 -> 0.03059
Loss epoch 8 -> 0.02527
Loss epoch 9 -> 0.01064
Loss epoch 10 -> 0.07188
Loss epoch 11 -> 0.04440
Loss epoch 12 -> 0.02436
Loss epoch 13 -> 0.03332
Loss epoch 14 -> 0.01567
Loss epoch 15 -> 0.04849
Loss epoch 16 -> 0.03571
Loss epoch 17 -> 0.03530
Loss epoch 18 -> 0.02081
Loss epoch 19 -> 0.02357
Loss epoch 20 -> 0.02514
Loss epoch 21 -> 0.01318
Loss epoch 22 -> 0.01072
Loss epoch 23 -> 0.01920
Loss epoch 24 -> 0.02749
Loss epoch 25 -> 0.02639
Loss epoch 26 -> 0.01152
Loss epoch 27 -> 0.01431
Loss epoch 28 -> 0.00548
Loss epoch 29 -> 0.01034
Loss epoch 30 -> 0.01297
Loss epoch 31 -> 0.02922
Loss epoch 32 -> 0.01333
Loss epoch 33 -> 0.01520
Loss epoch 34 -> 0.01823
Loss epoch 35 -> 0.01565
Loss epoch 36 -> 0.03844
Loss epoch 37 -> 0.02658
Loss epoch 38 -> 0.01317
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–…â–…â–ƒâ–„â–…â–‡â–‡â–…â–…â–†â–†â–…â–†â–„â–„â–…â–…â–…â–†â–…â–†â–‡â–‡â–…â–…â–†â–‡â–ˆâ–‡â–†â–…â–†â–†â–†â–†â–ƒâ–„â–ƒâ–…
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.89
wandb:    Train/Loss 0.01013
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 7 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/yrpw8vli
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_144024-yrpw8vli/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_152242-8xtb1qvr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 8
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/8xtb1qvr
Loss epoch 39 -> 0.01013
end...
Accuracy : 0.89
Classification report:
              precision    recall  f1-score   support

          ia       0.91      0.87      0.89       500
      nature       0.88      0.91      0.89       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 8
Training...
Loss epoch 0 -> 1.27188
Loss epoch 1 -> 0.32065
Loss epoch 2 -> 0.12001
Loss epoch 3 -> 0.05022
Loss epoch 4 -> 0.05940
Loss epoch 5 -> 0.04010
Loss epoch 6 -> 0.03335
Loss epoch 7 -> 0.02779
Loss epoch 8 -> 0.02876
Loss epoch 9 -> 0.02922
Loss epoch 10 -> 0.02245
Loss epoch 11 -> 0.02518
Loss epoch 12 -> 0.01910
Loss epoch 13 -> 0.02221
Loss epoch 14 -> 0.02209
Loss epoch 15 -> 0.02631
Loss epoch 16 -> 0.02011
Loss epoch 17 -> 0.01027
Loss epoch 18 -> 0.03276
Loss epoch 19 -> 0.08291
Loss epoch 20 -> 0.02791
Loss epoch 21 -> 0.01551
Loss epoch 22 -> 0.02308
Loss epoch 23 -> 0.00910
Loss epoch 24 -> 0.00325
Loss epoch 25 -> 0.00724
Loss epoch 26 -> 0.01261
Loss epoch 27 -> 0.00790
Loss epoch 28 -> 0.00359
Loss epoch 29 -> 0.00828
Loss epoch 30 -> 0.00648
Loss epoch 31 -> 0.00409
Loss epoch 32 -> 0.00765
Loss epoch 33 -> 0.01038
Loss epoch 34 -> 0.02342
Loss epoch 35 -> 0.02091
Loss epoch 36 -> 0.00767
Loss epoch 37 -> 0.01528
Loss epoch 38 -> 0.00902
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–…â–‡â–‡â–‡â–‡â–‡â–†â–†â–‡â–†â–‡â–†â–†â–†
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.891
wandb:    Train/Loss 0.00407
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 8 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/8xtb1qvr
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_152242-8xtb1qvr/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_160458-u5osjs8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 9
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/u5osjs8k
Loss epoch 39 -> 0.00407
end...
Accuracy : 0.891
Classification report:
              precision    recall  f1-score   support

          ia       0.89      0.89      0.89       500
      nature       0.89      0.90      0.89       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 9
Training...
Loss epoch 0 -> 1.08314
Loss epoch 1 -> 0.23202
Loss epoch 2 -> 0.08770
Loss epoch 3 -> 0.04361
Loss epoch 4 -> 0.03495
Loss epoch 5 -> 0.03328
Loss epoch 6 -> 0.03749
Loss epoch 7 -> 0.06787
Loss epoch 8 -> 0.03736
Loss epoch 9 -> 0.02820
Loss epoch 10 -> 0.03074
Loss epoch 11 -> 0.02327
Loss epoch 12 -> 0.03151
Loss epoch 13 -> 0.02915
Loss epoch 14 -> 0.01704
Loss epoch 15 -> 0.02845
Loss epoch 16 -> 0.05991
Loss epoch 17 -> 0.02953
Loss epoch 18 -> 0.02087
Loss epoch 19 -> 0.02466
Loss epoch 20 -> 0.01029
Loss epoch 21 -> 0.00516
Loss epoch 22 -> 0.02515
Loss epoch 23 -> 0.02572
Loss epoch 24 -> 0.00787
Loss epoch 25 -> 0.00821
Loss epoch 26 -> 0.00792
Loss epoch 27 -> 0.01051
Loss epoch 28 -> 0.01783
Loss epoch 29 -> 0.01631
Loss epoch 30 -> 0.00755
Loss epoch 31 -> 0.00210
Loss epoch 32 -> 0.00174
Loss epoch 33 -> 0.00311
Loss epoch 34 -> 0.00016
Loss epoch 35 -> 0.00005
Loss epoch 36 -> 0.00004
Loss epoch 37 -> 0.00003
Loss epoch 38 -> 0.00002
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–„â–†â–‡â–†â–‡â–‡â–…â–ˆâ–‡â–…â–…â–…â–…â–‡â–„â–…â–…â–…â–ˆâ–‡â–„â–…â–…â–…â–…â–†â–†â–…â–…â–„â–…â–…â–†â–…â–†â–†â–†â–†â–…
wandb:    Train/Loss â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.895
wandb:    Train/Loss 2e-05
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 9 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/u5osjs8k
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_160458-u5osjs8k/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250314_164724-o037ez21
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Training on midjourney dataset - Add tokens 10
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/o037ez21
Loss epoch 39 -> 0.00002
end...
Accuracy : 0.895
Classification report:
              precision    recall  f1-score   support

          ia       0.88      0.91      0.90       500
      nature       0.91      0.88      0.89       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.89      1000
weighted avg       0.90      0.90      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
Loss epoch 0 -> 1.05842
Loss epoch 1 -> 0.29941
Loss epoch 2 -> 0.11724
Loss epoch 3 -> 0.05162
Loss epoch 4 -> 0.05518
Loss epoch 5 -> 0.04297
Loss epoch 6 -> 0.01633
Loss epoch 7 -> 0.01651
Loss epoch 8 -> 0.01864
Loss epoch 9 -> 0.01906
Loss epoch 10 -> 0.03544
Loss epoch 11 -> 0.01801
Loss epoch 12 -> 0.01534
Loss epoch 13 -> 0.01451
Loss epoch 14 -> 0.01882
Loss epoch 15 -> 0.02510
Loss epoch 16 -> 0.02924
Loss epoch 17 -> 0.02719
Loss epoch 18 -> 0.01642
Loss epoch 19 -> 0.01219
Loss epoch 20 -> 0.01148
Loss epoch 21 -> 0.03771
Loss epoch 22 -> 0.02311
Loss epoch 23 -> 0.02867
Loss epoch 24 -> 0.02202
Loss epoch 25 -> 0.01232
Loss epoch 26 -> 0.01370
Loss epoch 27 -> 0.01070
Loss epoch 28 -> 0.00894
Loss epoch 29 -> 0.01093
Loss epoch 30 -> 0.01037
Loss epoch 31 -> 0.00917
Loss epoch 32 -> 0.00576
Loss epoch 33 -> 0.00508
Loss epoch 34 -> 0.01347
Loss epoch 35 -> 0.01800
Loss epoch 36 -> 0.02582
Loss epoch 37 -> 0.01763
Loss epoch 38 -> 0.01491
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–ˆâ–ˆâ–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.89
wandb:    Train/Loss 0.01743
wandb:         epoch 39
wandb: 
wandb: ðŸš€ View run Training on midjourney dataset - Add tokens 10 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/o037ez21
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250314_164724-o037ez21/logs
Loss epoch 39 -> 0.01743
end...
Accuracy : 0.89
Classification report:
              precision    recall  f1-score   support

          ia       0.87      0.92      0.89       500
      nature       0.91      0.86      0.89       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
