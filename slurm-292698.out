wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: manurslf (manurslf301) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250319_094108-d64eh8e6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 50 (20250319-094107)
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/d64eh8e6
Operation on cuda
Using midjourney DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 50
Training...
Loss epoch 0 -> 1.15776
Loss epoch 1 -> 0.34287
Loss epoch 2 -> 0.15380
Loss epoch 3 -> 0.03056
Loss epoch 4 -> 0.01741
Loss epoch 5 -> 0.00825
Loss epoch 6 -> 0.01093
Loss epoch 7 -> 0.00769
Loss epoch 8 -> 0.00626
Loss epoch 9 -> 0.03376
Loss epoch 10 -> 0.00731
Loss epoch 11 -> 0.00101
Loss epoch 12 -> 0.00009
Loss epoch 13 -> 0.00005
Loss epoch 14 -> 0.00004
Loss epoch 15 -> 0.00003
Loss epoch 16 -> 0.00003
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00001
Loss epoch 35 -> 0.00000
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–†â–ƒâ–†â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.91
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: ðŸš€ View run DS midjourney dataset - AT 50 (20250319-094107) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/d64eh8e6
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250319_094108-d64eh8e6/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250319_113136-hs71czvq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 100 (20250319-094107)
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/hs71czvq
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.91
Classification report:
              precision    recall  f1-score   support

          ia       0.92      0.90      0.91       500
      nature       0.90      0.92      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 100
Training...
Traceback (most recent call last):
  File "/home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/utils.py", line 248, in <module>
    plot_accuracy(configurations.MODEL,
  File "/home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/utils.py", line 215, in plot_accuracy
    model = training(model_d,
            ^^^^^^^^^^^^^^^^^
  File "/home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/utils.py", line 130, in training
    loss.backward()
  File "/home/users/r/rasolof2/miniconda3/envs/project/lib/python3.12/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/users/r/rasolof2/miniconda3/envs/project/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/users/r/rasolof2/miniconda3/envs/project/lib/python3.12/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 23.80 GiB of which 1.43 GiB is free. Including non-PyTorch memory, this process has 22.37 GiB memory in use. Of the allocated memory 21.20 GiB is allocated by PyTorch, and 878.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mDS midjourney dataset - AT 100 (20250319-094107)[0m at: [34mhttps://wandb.ai/manurslf301/Encoder-DecoderProject/runs/hs71czvq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250319_113136-hs71czvq/logs[0m
srun: error: gpu002: task 0: Exited with exit code 1
