wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: manurslf (manurslf301) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_082104-jfslb15w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 2 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/jfslb15w
Operation on cuda
Using midjourney DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 2
Training...
Loss epoch 0 -> 1.01503
Loss epoch 1 -> 0.32884
Loss epoch 2 -> 0.11346
Loss epoch 3 -> 0.02342
Loss epoch 4 -> 0.01230
Loss epoch 5 -> 0.00671
Loss epoch 6 -> 0.00386
Loss epoch 7 -> 0.00712
Loss epoch 8 -> 0.02103
Loss epoch 9 -> 0.02931
Loss epoch 10 -> 0.00294
Loss epoch 11 -> 0.00232
Loss epoch 12 -> 0.00085
Loss epoch 13 -> 0.00028
Loss epoch 14 -> 0.00012
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00000
Loss epoch 28 -> 0.00000
Loss epoch 29 -> 0.00000
Loss epoch 30 -> 0.00000
Loss epoch 31 -> 0.00000
Loss epoch 32 -> 0.00000
Loss epoch 33 -> 0.00000
Loss epoch 34 -> 0.00000
Loss epoch 35 -> 0.00000
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▂▇▆▁▇███████████████████████████████████
wandb:    Train/Loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.914
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 2 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/jfslb15w
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_082104-jfslb15w/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_100521-2ao224ty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 3 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/2ao224ty
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.914
Classification report:
              precision    recall  f1-score   support

          ia       0.92      0.91      0.91       500
      nature       0.91      0.92      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 3
Training...
Loss epoch 0 -> 0.92059
Loss epoch 1 -> 0.27141
Loss epoch 2 -> 0.11239
Loss epoch 3 -> 0.02508
Loss epoch 4 -> 0.01995
Loss epoch 5 -> 0.00783
Loss epoch 6 -> 0.00690
Loss epoch 7 -> 0.00248
Loss epoch 8 -> 0.00270
Loss epoch 9 -> 0.00487
Loss epoch 10 -> 0.00619
Loss epoch 11 -> 0.02357
Loss epoch 12 -> 0.00920
Loss epoch 13 -> 0.00210
Loss epoch 14 -> 0.00152
Loss epoch 15 -> 0.00023
Loss epoch 16 -> 0.00003
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00000
Loss epoch 33 -> 0.00000
Loss epoch 34 -> 0.00000
Loss epoch 35 -> 0.00000
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▄█▄▁▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:    Train/Loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇█████
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.908
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 3 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/2ao224ty
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_100521-2ao224ty/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_115003-tt1m9cqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 4 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/tt1m9cqc
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.908
Classification report:
              precision    recall  f1-score   support

          ia       0.92      0.89      0.91       500
      nature       0.90      0.92      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 4
Training...
Loss epoch 0 -> 1.03620
Loss epoch 1 -> 0.28576
Loss epoch 2 -> 0.10664
Loss epoch 3 -> 0.01702
Loss epoch 4 -> 0.02867
Loss epoch 5 -> 0.01640
Loss epoch 6 -> 0.01243
Loss epoch 7 -> 0.01414
Loss epoch 8 -> 0.01001
Loss epoch 9 -> 0.00358
Loss epoch 10 -> 0.00111
Loss epoch 11 -> 0.00187
Loss epoch 12 -> 0.00934
Loss epoch 13 -> 0.01430
Loss epoch 14 -> 0.00400
Loss epoch 15 -> 0.00424
Loss epoch 16 -> 0.00992
Loss epoch 17 -> 0.00937
Loss epoch 18 -> 0.01054
Loss epoch 19 -> 0.00557
Loss epoch 20 -> 0.00228
Loss epoch 21 -> 0.00428
Loss epoch 22 -> 0.00240
Loss epoch 23 -> 0.00067
Loss epoch 24 -> 0.00049
Loss epoch 25 -> 0.00021
Loss epoch 26 -> 0.00014
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00001
Loss epoch 35 -> 0.00001
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▁▇█▆▇▇▇▆▇▆▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆
wandb:    Train/Loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.901
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 4 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/tt1m9cqc
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_115003-tt1m9cqc/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_133453-aj2hw6hk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 5 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/aj2hw6hk
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.901
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.91      0.90       500
      nature       0.91      0.90      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 5
Training...
Loss epoch 0 -> 1.18022
Loss epoch 1 -> 0.32407
Loss epoch 2 -> 0.11522
Loss epoch 3 -> 0.02511
Loss epoch 4 -> 0.02247
Loss epoch 5 -> 0.01085
Loss epoch 6 -> 0.01187
Loss epoch 7 -> 0.00201
Loss epoch 8 -> 0.00175
Loss epoch 9 -> 0.00215
Loss epoch 10 -> 0.02432
Loss epoch 11 -> 0.01808
Loss epoch 12 -> 0.01333
Loss epoch 13 -> 0.01209
Loss epoch 14 -> 0.00655
Loss epoch 15 -> 0.00135
Loss epoch 16 -> 0.00045
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00000
Loss epoch 35 -> 0.00000
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▁▇▄▇▃▇████▇▇▇▇▇▇▇███████████████████████
wandb:    Train/Loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█████
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.914
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 5 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/aj2hw6hk
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_133453-aj2hw6hk/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_151916-vfyow5k0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 6 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/vfyow5k0
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.914
Classification report:
              precision    recall  f1-score   support

          ia       0.93      0.89      0.91       500
      nature       0.90      0.94      0.92       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 6
Training...
Loss epoch 0 -> 1.24949
Loss epoch 1 -> 0.33816
Loss epoch 2 -> 0.12847
Loss epoch 3 -> 0.02762
Loss epoch 4 -> 0.00688
Loss epoch 5 -> 0.00409
Loss epoch 6 -> 0.02224
Loss epoch 7 -> 0.01713
Loss epoch 8 -> 0.00545
Loss epoch 9 -> 0.00756
Loss epoch 10 -> 0.00244
Loss epoch 11 -> 0.00809
Loss epoch 12 -> 0.00639
Loss epoch 13 -> 0.01260
Loss epoch 14 -> 0.00935
Loss epoch 15 -> 0.00194
Loss epoch 16 -> 0.00033
Loss epoch 17 -> 0.00004
Loss epoch 18 -> 0.00003
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00002
Loss epoch 21 -> 0.00002
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00001
Loss epoch 35 -> 0.00001
Loss epoch 36 -> 0.00001
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▁█▃▅▅▅▆▅▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:    Train/Loss █▂▇▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.908
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 6 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/vfyow5k0
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_151916-vfyow5k0/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_170416-js2u54v6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 7 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/js2u54v6
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.908
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.92      0.91       500
      nature       0.92      0.89      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 7
Training...
Loss epoch 0 -> 1.08243
Loss epoch 1 -> 0.33055
Loss epoch 2 -> 0.14634
Loss epoch 3 -> 0.03120
Loss epoch 4 -> 0.00783
Loss epoch 5 -> 0.00387
Loss epoch 6 -> 0.00074
Loss epoch 7 -> 0.00201
Loss epoch 8 -> 0.03309
Loss epoch 9 -> 0.01216
Loss epoch 10 -> 0.00391
Loss epoch 11 -> 0.00612
Loss epoch 12 -> 0.02030
Loss epoch 13 -> 0.04087
Loss epoch 14 -> 0.01172
Loss epoch 15 -> 0.00413
Loss epoch 16 -> 0.00091
Loss epoch 17 -> 0.00011
Loss epoch 18 -> 0.00004
Loss epoch 19 -> 0.00003
Loss epoch 20 -> 0.00003
Loss epoch 21 -> 0.00003
Loss epoch 22 -> 0.00002
Loss epoch 23 -> 0.00002
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00002
Loss epoch 26 -> 0.00002
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00001
Loss epoch 35 -> 0.00001
Loss epoch 36 -> 0.00001
Loss epoch 37 -> 0.00001
Loss epoch 38 -> 0.00001
Loss epoch 39 -> 0.00001
Loss epoch 40 -> 0.00001
Loss epoch 41 -> 0.00001
Loss epoch 42 -> 0.00001
Loss epoch 43 -> 0.00001
Loss epoch 44 -> 0.00001
Loss epoch 45 -> 0.00001
Loss epoch 46 -> 0.00001
Loss epoch 47 -> 0.00001
Loss epoch 48 -> 0.00001
Loss epoch 49 -> 0.00001
Loss epoch 50 -> 0.00001
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb: uploading history steps 99-99, summary, console lines 101-102
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▁▅▅▄▄▄▆▇████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:    Train/Loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▇▇▇█████
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.914
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 7 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/js2u54v6
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_170416-js2u54v6/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_184852-hms1nbdo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 8 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/hms1nbdo
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.914
Classification report:
              precision    recall  f1-score   support

          ia       0.91      0.92      0.91       500
      nature       0.92      0.91      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 8
Training...
Loss epoch 0 -> 1.19386
Loss epoch 1 -> 0.32516
Loss epoch 2 -> 0.10919
Loss epoch 3 -> 0.03064
Loss epoch 4 -> 0.00644
Loss epoch 5 -> 0.00955
Loss epoch 6 -> 0.01046
Loss epoch 7 -> 0.00650
Loss epoch 8 -> 0.02295
Loss epoch 9 -> 0.03176
Loss epoch 10 -> 0.00561
Loss epoch 11 -> 0.00524
Loss epoch 12 -> 0.01169
Loss epoch 13 -> 0.00503
Loss epoch 14 -> 0.00170
Loss epoch 15 -> 0.00363
Loss epoch 16 -> 0.00599
Loss epoch 17 -> 0.01116
Loss epoch 18 -> 0.00968
Loss epoch 19 -> 0.00856
Loss epoch 20 -> 0.00346
Loss epoch 21 -> 0.00186
Loss epoch 22 -> 0.00037
Loss epoch 23 -> 0.00006
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00001
Loss epoch 35 -> 0.00001
Loss epoch 36 -> 0.00001
Loss epoch 37 -> 0.00001
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy █▄▂▃▅▄▁▂▇▄▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:    Train/Loss ▃▂▆█▂▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.91
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 8 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/hms1nbdo
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_184852-hms1nbdo/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_203320-1ubw5s6r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 9 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/1ubw5s6r
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.91
Classification report:
              precision    recall  f1-score   support

          ia       0.91      0.91      0.91       500
      nature       0.91      0.91      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 9
Training...
Loss epoch 0 -> 1.23215
Loss epoch 1 -> 0.35384
Loss epoch 2 -> 0.13720
Loss epoch 3 -> 0.02866
Loss epoch 4 -> 0.00830
Loss epoch 5 -> 0.00446
Loss epoch 6 -> 0.00608
Loss epoch 7 -> 0.01736
Loss epoch 8 -> 0.02657
Loss epoch 9 -> 0.00900
Loss epoch 10 -> 0.00713
Loss epoch 11 -> 0.00767
Loss epoch 12 -> 0.00355
Loss epoch 13 -> 0.00030
Loss epoch 14 -> 0.00004
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00000
Loss epoch 31 -> 0.00000
Loss epoch 32 -> 0.00000
Loss epoch 33 -> 0.00000
Loss epoch 34 -> 0.00000
Loss epoch 35 -> 0.00000
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▁▆▆▆█▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:    Train/Loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.908
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 9 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/1ubw5s6r
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_203320-1ubw5s6r/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250318_221721-vsepyfca
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 10 (20250318-082104)
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/vsepyfca
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.908
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.92      0.91       500
      nature       0.92      0.90      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
Loss epoch 0 -> 1.22201
Loss epoch 1 -> 0.37289
Loss epoch 2 -> 0.14945
Loss epoch 3 -> 0.03037
Loss epoch 4 -> 0.00920
Loss epoch 5 -> 0.00583
Loss epoch 6 -> 0.00595
Loss epoch 7 -> 0.00273
Loss epoch 8 -> 0.01133
Loss epoch 9 -> 0.02936
Loss epoch 10 -> 0.01026
Loss epoch 11 -> 0.00666
Loss epoch 12 -> 0.00488
Loss epoch 13 -> 0.00156
Loss epoch 14 -> 0.00062
Loss epoch 15 -> 0.00010
Loss epoch 16 -> 0.00003
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00001
Loss epoch 35 -> 0.00001
Loss epoch 36 -> 0.00001
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy ▁▆▅█▆▆███████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:    Train/Loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         epoch ▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇█████
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.915
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: 🚀 View run DS midjourney dataset - AT 10 (20250318-082104) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/vsepyfca
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250318_221721-vsepyfca/logs
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.915
Classification report:
              precision    recall  f1-score   support

          ia       0.92      0.91      0.91       500
      nature       0.91      0.92      0.92       500

    accuracy                           0.92      1000
   macro avg       0.92      0.92      0.91      1000
weighted avg       0.92      0.92      0.91      1000

-----------------------------------------------------------------------------------------------------------
