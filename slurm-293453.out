wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: manurslf (manurslf301) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250321_100903-x95l4g6t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ACC_TOK 20250321-100902
wandb: ⭐️ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: 🚀 View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/x95l4g6t
Operation on cuda
Using biggan DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
Loss epoch 0 -> 0.14687
Loss epoch 1 -> 0.00918
Loss epoch 2 -> 0.00246
Loss epoch 3 -> 0.00232
Loss epoch 4 -> 0.00101
Loss epoch 5 -> 0.00014
Loss epoch 6 -> 0.00003
Loss epoch 7 -> 0.00002
Loss epoch 8 -> 0.00002
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00002
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.99
Classification report:
              precision    recall  f1-score   support

          ia       0.99      0.99      0.99       500
      nature       0.99      0.99      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
wandb: WARNING Tried to log to step 0 that is less than the current step 30. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.75709
Loss epoch 1 -> 0.02498
Loss epoch 2 -> 0.00456
Loss epoch 3 -> 0.00064
Loss epoch 4 -> 0.00011
Loss epoch 5 -> 0.00006
Loss epoch 6 -> 0.00004
Loss epoch 7 -> 0.00003
Loss epoch 8 -> 0.00003
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00002
Loss epoch 12 -> 0.00002
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.989
Classification report:
              precision    recall  f1-score   support

          ia       0.99      0.98      0.99       500
      nature       0.98      0.99      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 30
Training...
wandb: WARNING Tried to log to step 10 that is less than the current step 60. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.79480
Loss epoch 1 -> 0.03405
Loss epoch 2 -> 0.00562
Loss epoch 3 -> 0.00144
Loss epoch 4 -> 0.00022
Loss epoch 5 -> 0.00007
Loss epoch 6 -> 0.00005
Loss epoch 7 -> 0.00004
Loss epoch 8 -> 0.00003
Loss epoch 9 -> 0.00003
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00002
Loss epoch 12 -> 0.00002
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.992
Classification report:
              precision    recall  f1-score   support

          ia       1.00      0.99      0.99       500
      nature       0.99      1.00      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
wandb: WARNING Tried to log to step 30 that is less than the current step 90. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.73151
Loss epoch 1 -> 0.02948
Loss epoch 2 -> 0.00416
Loss epoch 3 -> 0.00035
Loss epoch 4 -> 0.00007
Loss epoch 5 -> 0.00005
Loss epoch 6 -> 0.00003
Loss epoch 7 -> 0.00003
Loss epoch 8 -> 0.00002
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00001
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.991
Classification report:
              precision    recall  f1-score   support

          ia       0.99      0.99      0.99       500
      nature       0.99      0.99      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Operation on cuda
Using vqdm DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
wandb: WARNING Tried to log to step 50 that is less than the current step 120. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.22561
Loss epoch 1 -> 0.03372
Loss epoch 2 -> 0.01157
Loss epoch 3 -> 0.01136
Loss epoch 4 -> 0.00655
Loss epoch 5 -> 0.00359
Loss epoch 6 -> 0.00196
Loss epoch 7 -> 0.00141
Loss epoch 8 -> 0.00138
Loss epoch 9 -> 0.00024
Loss epoch 10 -> 0.00004
Loss epoch 11 -> 0.00003
Loss epoch 12 -> 0.00003
Loss epoch 13 -> 0.00003
Loss epoch 14 -> 0.00002
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00002
Loss epoch 21 -> 0.00002
Loss epoch 22 -> 0.00002
Loss epoch 23 -> 0.00002
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00002
Loss epoch 26 -> 0.00002
Loss epoch 27 -> 0.00002
Loss epoch 28 -> 0.00002
Loss epoch 29 -> 0.00002
end...
Accuracy : 0.979
Classification report:
              precision    recall  f1-score   support

          ia       0.97      0.99      0.98       500
      nature       0.99      0.97      0.98       500

    accuracy                           0.98      1000
   macro avg       0.98      0.98      0.98      1000
weighted avg       0.98      0.98      0.98      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
wandb: WARNING Tried to log to step 0 that is less than the current step 150. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.91392
Loss epoch 1 -> 0.08083
Loss epoch 2 -> 0.01378
Loss epoch 3 -> 0.00162
Loss epoch 4 -> 0.00039
Loss epoch 5 -> 0.00195
Loss epoch 6 -> 0.00014
Loss epoch 7 -> 0.00004
Loss epoch 8 -> 0.00003
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00002
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.979
Classification report:
              precision    recall  f1-score   support

          ia       0.98      0.97      0.98       500
      nature       0.97      0.98      0.98       500

    accuracy                           0.98      1000
   macro avg       0.98      0.98      0.98      1000
weighted avg       0.98      0.98      0.98      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 30
Training...
wandb: WARNING Tried to log to step 10 that is less than the current step 180. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.79808
Loss epoch 1 -> 0.08013
Loss epoch 2 -> 0.01899
Loss epoch 3 -> 0.00581
Loss epoch 4 -> 0.00065
Loss epoch 5 -> 0.00209
Loss epoch 6 -> 0.00144
Loss epoch 7 -> 0.00531
Loss epoch 8 -> 0.00265
Loss epoch 9 -> 0.00112
Loss epoch 10 -> 0.00061
Loss epoch 11 -> 0.00108
Loss epoch 12 -> 0.00007
Loss epoch 13 -> 0.00002
Loss epoch 14 -> 0.00002
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.986
Classification report:
              precision    recall  f1-score   support

          ia       0.98      0.99      0.99       500
      nature       0.99      0.98      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
wandb: WARNING Tried to log to step 30 that is less than the current step 210. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.84870
Loss epoch 1 -> 0.07384
Loss epoch 2 -> 0.01806
Loss epoch 3 -> 0.00709
Loss epoch 4 -> 0.00141
Loss epoch 5 -> 0.00067
Loss epoch 6 -> 0.00025
Loss epoch 7 -> 0.00006
Loss epoch 8 -> 0.00002
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00001
Loss epoch 11 -> 0.00001
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
wandb: WARNING Tried to log to step 50 that is less than the current step 240. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.982
Classification report:
              precision    recall  f1-score   support

          ia       0.97      0.99      0.98       500
      nature       0.99      0.97      0.98       500

    accuracy                           0.98      1000
   macro avg       0.98      0.98      0.98      1000
weighted avg       0.98      0.98      0.98      1000

-----------------------------------------------------------------------------------------------------------
Operation on cuda
Using sdv5 DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
Loss epoch 0 -> 0.42751
Loss epoch 1 -> 0.09785
Loss epoch 2 -> 0.03735
Loss epoch 3 -> 0.02805
Loss epoch 4 -> 0.01327
Loss epoch 5 -> 0.00554
Loss epoch 6 -> 0.00118
Loss epoch 7 -> 0.00222
Loss epoch 8 -> 0.00062
Loss epoch 9 -> 0.00022
Loss epoch 10 -> 0.00014
Loss epoch 11 -> 0.00012
Loss epoch 12 -> 0.00010
Loss epoch 13 -> 0.00009
Loss epoch 14 -> 0.00009
Loss epoch 15 -> 0.00008
Loss epoch 16 -> 0.00007
Loss epoch 17 -> 0.00007
Loss epoch 18 -> 0.00007
Loss epoch 19 -> 0.00006
Loss epoch 20 -> 0.00006
Loss epoch 21 -> 0.00006
Loss epoch 22 -> 0.00006
Loss epoch 23 -> 0.00006
Loss epoch 24 -> 0.00006
Loss epoch 25 -> 0.00006
Loss epoch 26 -> 0.00006
Loss epoch 27 -> 0.00006
Loss epoch 28 -> 0.00006
Loss epoch 29 -> 0.00006
end...
Accuracy : 0.886
Classification report:
              precision    recall  f1-score   support

          ia       0.88      0.90      0.89       500
      nature       0.90      0.87      0.88       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
wandb: WARNING Tried to log to step 0 that is less than the current step 270. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.12988
Loss epoch 1 -> 0.28537
Loss epoch 2 -> 0.10572
Loss epoch 3 -> 0.02320
Loss epoch 4 -> 0.00582
Loss epoch 5 -> 0.00146
Loss epoch 6 -> 0.00222
Loss epoch 7 -> 0.00989
Loss epoch 8 -> 0.01711
Loss epoch 9 -> 0.00172
Loss epoch 10 -> 0.00012
Loss epoch 11 -> 0.00004
Loss epoch 12 -> 0.00003
Loss epoch 13 -> 0.00002
Loss epoch 14 -> 0.00002
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.908
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.92      0.91       500
      nature       0.92      0.90      0.91       500

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 30
Training...
wandb: WARNING Tried to log to step 10 that is less than the current step 300. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.87225
Loss epoch 1 -> 0.25801
Loss epoch 2 -> 0.08320
Loss epoch 3 -> 0.02317
Loss epoch 4 -> 0.01245
Loss epoch 5 -> 0.01546
Loss epoch 6 -> 0.01005
Loss epoch 7 -> 0.00209
Loss epoch 8 -> 0.00024
Loss epoch 9 -> 0.00004
Loss epoch 10 -> 0.00003
Loss epoch 11 -> 0.00003
Loss epoch 12 -> 0.00002
Loss epoch 13 -> 0.00002
Loss epoch 14 -> 0.00002
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
wandb: WARNING Tried to log to step 30 that is less than the current step 330. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.897
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.90      0.90       500
      nature       0.90      0.90      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
Loss epoch 0 -> 1.14461
Loss epoch 1 -> 0.28573
Loss epoch 2 -> 0.11707
Loss epoch 3 -> 0.02105
Loss epoch 4 -> 0.01250
Loss epoch 5 -> 0.01149
Loss epoch 6 -> 0.00652
Loss epoch 7 -> 0.00334
Loss epoch 8 -> 0.00394
Loss epoch 9 -> 0.00130
Loss epoch 10 -> 0.00019
Loss epoch 11 -> 0.00003
Loss epoch 12 -> 0.00002
Loss epoch 13 -> 0.00002
Loss epoch 14 -> 0.00002
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.895
Classification report:
              precision    recall  f1-score   support

          ia       0.89      0.90      0.90       500
      nature       0.90      0.89      0.89       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.89      1000
weighted avg       0.90      0.90      0.89      1000

-----------------------------------------------------------------------------------------------------------
Operation on cuda
Using wukong DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
wandb: WARNING Tried to log to step 50 that is less than the current step 360. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.41002
Loss epoch 1 -> 0.10026
Loss epoch 2 -> 0.02727
Loss epoch 3 -> 0.01421
Loss epoch 4 -> 0.00774
Loss epoch 5 -> 0.00527
Loss epoch 6 -> 0.00051
Loss epoch 7 -> 0.00018
Loss epoch 8 -> 0.00013
Loss epoch 9 -> 0.00011
Loss epoch 10 -> 0.00009
Loss epoch 11 -> 0.00009
Loss epoch 12 -> 0.00008
Loss epoch 13 -> 0.00007
Loss epoch 14 -> 0.00007
Loss epoch 15 -> 0.00006
Loss epoch 16 -> 0.00006
Loss epoch 17 -> 0.00006
Loss epoch 18 -> 0.00005
Loss epoch 19 -> 0.00005
Loss epoch 20 -> 0.00005
Loss epoch 21 -> 0.00005
Loss epoch 22 -> 0.00005
Loss epoch 23 -> 0.00005
Loss epoch 24 -> 0.00005
Loss epoch 25 -> 0.00005
Loss epoch 26 -> 0.00005
Loss epoch 27 -> 0.00005
Loss epoch 28 -> 0.00005
Loss epoch 29 -> 0.00005
end...
Accuracy : 0.903
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.90      0.90       500
      nature       0.90      0.90      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
wandb: WARNING Tried to log to step 0 that is less than the current step 390. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.07919
Loss epoch 1 -> 0.28755
Loss epoch 2 -> 0.09011
Loss epoch 3 -> 0.02220
Loss epoch 4 -> 0.00983
Loss epoch 5 -> 0.00847
Loss epoch 6 -> 0.00901
Loss epoch 7 -> 0.00580
Loss epoch 8 -> 0.00986
Loss epoch 9 -> 0.00482
Loss epoch 10 -> 0.00057
Loss epoch 11 -> 0.00008
Loss epoch 12 -> 0.00004
Loss epoch 13 -> 0.00003
Loss epoch 14 -> 0.00003
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00002
Loss epoch 21 -> 0.00002
Loss epoch 22 -> 0.00002
Loss epoch 23 -> 0.00002
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.899
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.90      0.90       500
      nature       0.90      0.90      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 30
Training...
wandb: WARNING Tried to log to step 10 that is less than the current step 420. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.30618
Loss epoch 1 -> 0.32900
Loss epoch 2 -> 0.12198
Loss epoch 3 -> 0.02350
Loss epoch 4 -> 0.01205
Loss epoch 5 -> 0.02120
Loss epoch 6 -> 0.00783
Loss epoch 7 -> 0.00573
Loss epoch 8 -> 0.00350
Loss epoch 9 -> 0.00049
Loss epoch 10 -> 0.00006
Loss epoch 11 -> 0.00004
Loss epoch 12 -> 0.00003
Loss epoch 13 -> 0.00002
Loss epoch 14 -> 0.00002
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.903
Classification report:
              precision    recall  f1-score   support

          ia       0.91      0.90      0.90       500
      nature       0.90      0.91      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
wandb: WARNING Tried to log to step 30 that is less than the current step 450. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.11696
Loss epoch 1 -> 0.27061
Loss epoch 2 -> 0.10601
Loss epoch 3 -> 0.02580
Loss epoch 4 -> 0.00577
Loss epoch 5 -> 0.00212
Loss epoch 6 -> 0.00143
Loss epoch 7 -> 0.01325
Loss epoch 8 -> 0.00942
Loss epoch 9 -> 0.00069
Loss epoch 10 -> 0.00015
Loss epoch 11 -> 0.00005
Loss epoch 12 -> 0.00004
Loss epoch 13 -> 0.00003
Loss epoch 14 -> 0.00003
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00002
Loss epoch 21 -> 0.00002
Loss epoch 22 -> 0.00002
Loss epoch 23 -> 0.00002
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00002
Loss epoch 26 -> 0.00002
Loss epoch 27 -> 0.00002
Loss epoch 28 -> 0.00002
wandb: WARNING Tried to log to step 50 that is less than the current step 480. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 29 -> 0.00002
end...
Accuracy : 0.902
Classification report:
              precision    recall  f1-score   support

          ia       0.91      0.89      0.90       500
      nature       0.90      0.91      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Operation on cuda
Using adm DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
Loss epoch 0 -> 0.16667
Loss epoch 1 -> 0.01151
Loss epoch 2 -> 0.01994
Loss epoch 3 -> 0.00228
Loss epoch 4 -> 0.00046
Loss epoch 5 -> 0.00006
Loss epoch 6 -> 0.00004
Loss epoch 7 -> 0.00003
Loss epoch 8 -> 0.00002
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00002
Loss epoch 12 -> 0.00002
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.992
Classification report:
              precision    recall  f1-score   support

          ia       0.99      0.99      0.99       500
      nature       0.99      0.99      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
wandb: WARNING Tried to log to step 0 that is less than the current step 510. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.69213
Loss epoch 1 -> 0.03688
Loss epoch 2 -> 0.01795
Loss epoch 3 -> 0.00482
Loss epoch 4 -> 0.00074
Loss epoch 5 -> 0.00012
Loss epoch 6 -> 0.00005
Loss epoch 7 -> 0.00003
Loss epoch 8 -> 0.00002
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00001
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.991
Classification report:
              precision    recall  f1-score   support

          ia       0.99      1.00      0.99       500
      nature       1.00      0.99      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 30
Training...
wandb: WARNING Tried to log to step 10 that is less than the current step 540. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.67803
Loss epoch 1 -> 0.03074
Loss epoch 2 -> 0.01063
Loss epoch 3 -> 0.00277
Loss epoch 4 -> 0.00016
Loss epoch 5 -> 0.00003
Loss epoch 6 -> 0.00001
Loss epoch 7 -> 0.00001
Loss epoch 8 -> 0.00001
Loss epoch 9 -> 0.00001
Loss epoch 10 -> 0.00001
Loss epoch 11 -> 0.00001
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00000
Loss epoch 14 -> 0.00000
Loss epoch 15 -> 0.00000
Loss epoch 16 -> 0.00000
Loss epoch 17 -> 0.00000
Loss epoch 18 -> 0.00000
Loss epoch 19 -> 0.00000
Loss epoch 20 -> 0.00000
Loss epoch 21 -> 0.00000
Loss epoch 22 -> 0.00000
Loss epoch 23 -> 0.00000
Loss epoch 24 -> 0.00000
Loss epoch 25 -> 0.00000
Loss epoch 26 -> 0.00000
Loss epoch 27 -> 0.00000
Loss epoch 28 -> 0.00000
Loss epoch 29 -> 0.00000
end...
Accuracy : 0.989
Classification report:
              precision    recall  f1-score   support

          ia       0.98      0.99      0.99       500
      nature       0.99      0.98      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
wandb: WARNING Tried to log to step 30 that is less than the current step 570. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.60289
Loss epoch 1 -> 0.02117
Loss epoch 2 -> 0.00817
Loss epoch 3 -> 0.00401
Loss epoch 4 -> 0.00701
Loss epoch 5 -> 0.00160
Loss epoch 6 -> 0.00013
Loss epoch 7 -> 0.00003
Loss epoch 8 -> 0.00001
Loss epoch 9 -> 0.00001
Loss epoch 10 -> 0.00001
Loss epoch 11 -> 0.00001
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00000
Loss epoch 19 -> 0.00000
Loss epoch 20 -> 0.00000
Loss epoch 21 -> 0.00000
Loss epoch 22 -> 0.00000
Loss epoch 23 -> 0.00000
Loss epoch 24 -> 0.00000
Loss epoch 25 -> 0.00000
Loss epoch 26 -> 0.00000
Loss epoch 27 -> 0.00000
Loss epoch 28 -> 0.00000
Loss epoch 29 -> 0.00000
end...
Accuracy : 0.994
Classification report:
              precision    recall  f1-score   support

          ia       0.99      1.00      0.99       500
      nature       1.00      0.99      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000

-----------------------------------------------------------------------------------------------------------
Operation on cuda
Using glide DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
wandb: WARNING Tried to log to step 50 that is less than the current step 600. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.31952
Loss epoch 1 -> 0.06047
Loss epoch 2 -> 0.01782
Loss epoch 3 -> 0.01024
Loss epoch 4 -> 0.00622
Loss epoch 5 -> 0.00133
Loss epoch 6 -> 0.00028
Loss epoch 7 -> 0.00013
Loss epoch 8 -> 0.00010
Loss epoch 9 -> 0.00008
Loss epoch 10 -> 0.00008
Loss epoch 11 -> 0.00007
Loss epoch 12 -> 0.00006
Loss epoch 13 -> 0.00005
Loss epoch 14 -> 0.00005
Loss epoch 15 -> 0.00005
Loss epoch 16 -> 0.00004
Loss epoch 17 -> 0.00004
Loss epoch 18 -> 0.00004
Loss epoch 19 -> 0.00004
Loss epoch 20 -> 0.00004
Loss epoch 21 -> 0.00004
Loss epoch 22 -> 0.00004
Loss epoch 23 -> 0.00004
Loss epoch 24 -> 0.00004
Loss epoch 25 -> 0.00004
Loss epoch 26 -> 0.00004
Loss epoch 27 -> 0.00004
Loss epoch 28 -> 0.00004
Loss epoch 29 -> 0.00003
end...
Accuracy : 0.938
Classification report:
              precision    recall  f1-score   support

          ia       0.93      0.94      0.94       500
      nature       0.94      0.93      0.94       500

    accuracy                           0.94      1000
   macro avg       0.94      0.94      0.94      1000
weighted avg       0.94      0.94      0.94      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
wandb: WARNING Tried to log to step 0 that is less than the current step 630. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.00777
Loss epoch 1 -> 0.15456
Loss epoch 2 -> 0.02841
Loss epoch 3 -> 0.00518
Loss epoch 4 -> 0.00152
Loss epoch 5 -> 0.00046
Loss epoch 6 -> 0.00725
Loss epoch 7 -> 0.01290
Loss epoch 8 -> 0.00801
Loss epoch 9 -> 0.00236
Loss epoch 10 -> 0.00384
Loss epoch 11 -> 0.00289
Loss epoch 12 -> 0.00026
Loss epoch 13 -> 0.00005
Loss epoch 14 -> 0.00003
Loss epoch 15 -> 0.00003
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00002
Loss epoch 21 -> 0.00002
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.948
Classification report:
              precision    recall  f1-score   support

          ia       0.95      0.94      0.95       500
      nature       0.94      0.95      0.95       500

    accuracy                           0.95      1000
   macro avg       0.95      0.95      0.95      1000
weighted avg       0.95      0.95      0.95      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 30
Training...
wandb: WARNING Tried to log to step 10 that is less than the current step 660. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.99107
Loss epoch 1 -> 0.15269
Loss epoch 2 -> 0.03645
Loss epoch 3 -> 0.00591
Loss epoch 4 -> 0.00038
Loss epoch 5 -> 0.00006
Loss epoch 6 -> 0.00003
Loss epoch 7 -> 0.00003
Loss epoch 8 -> 0.00002
Loss epoch 9 -> 0.00002
Loss epoch 10 -> 0.00002
Loss epoch 11 -> 0.00001
Loss epoch 12 -> 0.00001
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.957
Classification report:
              precision    recall  f1-score   support

          ia       0.95      0.97      0.96       500
      nature       0.97      0.95      0.96       500

    accuracy                           0.96      1000
   macro avg       0.96      0.96      0.96      1000
weighted avg       0.96      0.96      0.96      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
wandb: WARNING Tried to log to step 30 that is less than the current step 690. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.04045
Loss epoch 1 -> 0.15730
Loss epoch 2 -> 0.03132
Loss epoch 3 -> 0.02118
Loss epoch 4 -> 0.00814
Loss epoch 5 -> 0.00369
Loss epoch 6 -> 0.00306
Loss epoch 7 -> 0.00539
Loss epoch 8 -> 0.00298
Loss epoch 9 -> 0.00095
Loss epoch 10 -> 0.00122
Loss epoch 11 -> 0.00052
Loss epoch 12 -> 0.00006
Loss epoch 13 -> 0.00001
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.948
Classification report:
              precision    recall  f1-score   support

          ia       0.94      0.95      0.95       500
      nature       0.95      0.94      0.95       500

    accuracy                           0.95      1000
   macro avg       0.95      0.95      0.95      1000
weighted avg       0.95      0.95      0.95      1000

-----------------------------------------------------------------------------------------------------------
Operation on cuda
Using midjourney DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
wandb: WARNING Tried to log to step 50 that is less than the current step 720. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 0.45508
Loss epoch 1 -> 0.10916
Loss epoch 2 -> 0.02654
Loss epoch 3 -> 0.00473
Loss epoch 4 -> 0.00159
Loss epoch 5 -> 0.00044
Loss epoch 6 -> 0.00024
Loss epoch 7 -> 0.00015
Loss epoch 8 -> 0.00013
Loss epoch 9 -> 0.00011
Loss epoch 10 -> 0.00010
Loss epoch 11 -> 0.00009
Loss epoch 12 -> 0.00008
Loss epoch 13 -> 0.00008
Loss epoch 14 -> 0.00007
Loss epoch 15 -> 0.00007
Loss epoch 16 -> 0.00006
Loss epoch 17 -> 0.00006
Loss epoch 18 -> 0.00006
Loss epoch 19 -> 0.00005
Loss epoch 20 -> 0.00005
Loss epoch 21 -> 0.00005
Loss epoch 22 -> 0.00005
Loss epoch 23 -> 0.00005
Loss epoch 24 -> 0.00005
Loss epoch 25 -> 0.00005
Loss epoch 26 -> 0.00005
Loss epoch 27 -> 0.00005
Loss epoch 28 -> 0.00005
Loss epoch 29 -> 0.00005
end...
Accuracy : 0.885
Classification report:
              precision    recall  f1-score   support

          ia       0.87      0.90      0.89       500
      nature       0.90      0.87      0.88       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.88      1000
weighted avg       0.89      0.89      0.88      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
wandb: WARNING Tried to log to step 0 that is less than the current step 750. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.05612
Loss epoch 1 -> 0.28532
Loss epoch 2 -> 0.09736
Loss epoch 3 -> 0.02008
Loss epoch 4 -> 0.01434
Loss epoch 5 -> 0.00691
Loss epoch 6 -> 0.00259
Loss epoch 7 -> 0.00603
Loss epoch 8 -> 0.00380
Loss epoch 9 -> 0.00318
Loss epoch 10 -> 0.00295
Loss epoch 11 -> 0.00418
Loss epoch 12 -> 0.00558
Loss epoch 13 -> 0.00105
Loss epoch 14 -> 0.00013
Loss epoch 15 -> 0.00004
Loss epoch 16 -> 0.00003
Loss epoch 17 -> 0.00003
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00002
Loss epoch 21 -> 0.00002
Loss epoch 22 -> 0.00002
Loss epoch 23 -> 0.00002
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00002
Loss epoch 26 -> 0.00002
Loss epoch 27 -> 0.00002
Loss epoch 28 -> 0.00002
wandb: WARNING Tried to log to step 10 that is less than the current step 780. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 29 -> 0.00002
end...
Accuracy : 0.894
Classification report:
              precision    recall  f1-score   support

          ia       0.89      0.90      0.89       500
      nature       0.90      0.89      0.89       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 30
Training...
Loss epoch 0 -> 1.10673
Loss epoch 1 -> 0.33821
Loss epoch 2 -> 0.14944
Loss epoch 3 -> 0.02945
Loss epoch 4 -> 0.02067
Loss epoch 5 -> 0.01424
Loss epoch 6 -> 0.00339
Loss epoch 7 -> 0.00021
Loss epoch 8 -> 0.00008
Loss epoch 9 -> 0.00006
Loss epoch 10 -> 0.00005
Loss epoch 11 -> 0.00004
Loss epoch 12 -> 0.00003
Loss epoch 13 -> 0.00003
Loss epoch 14 -> 0.00003
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00002
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00002
Loss epoch 21 -> 0.00002
Loss epoch 22 -> 0.00002
Loss epoch 23 -> 0.00002
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00002
Loss epoch 26 -> 0.00002
Loss epoch 27 -> 0.00002
Loss epoch 28 -> 0.00002
Loss epoch 29 -> 0.00002
end...
Accuracy : 0.881
Classification report:
              precision    recall  f1-score   support

          ia       0.89      0.87      0.88       500
      nature       0.87      0.89      0.88       500

    accuracy                           0.88      1000
   macro avg       0.88      0.88      0.88      1000
weighted avg       0.88      0.88      0.88      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
wandb: WARNING Tried to log to step 30 that is less than the current step 810. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 0 -> 1.24696
Loss epoch 1 -> 0.37103
Loss epoch 2 -> 0.14761
Loss epoch 3 -> 0.04000
Loss epoch 4 -> 0.02386
Loss epoch 5 -> 0.00543
Loss epoch 6 -> 0.00053
Loss epoch 7 -> 0.00010
Loss epoch 8 -> 0.00004
Loss epoch 9 -> 0.00003
Loss epoch 10 -> 0.00003
Loss epoch 11 -> 0.00002
Loss epoch 12 -> 0.00002
Loss epoch 13 -> 0.00002
Loss epoch 14 -> 0.00002
Loss epoch 15 -> 0.00002
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy █▇██▇▇▇██▇▂▂▂▂▂▂▂▂▂▂▂▂▂███████▅▅▅▅▆▁▁▂▁▃
wandb:    Train/Loss ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb:         epoch ▃▇▇▂▃▆▇▃▅▄▆▇▇███▄▇█▃▄▄▇█▂▄▅▆▇▄▁▂▇█▂▄▆▁▆▅
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.917
wandb:    Train/Loss 1e-05
wandb:         epoch 29
wandb: 
wandb: 🚀 View run ACC_TOK 20250321-100902 at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/x95l4g6t
wandb: ⭐️ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250321_100903-x95l4g6t/logs
wandb: WARNING Tried to log to step 50 that is less than the current step 840. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Loss epoch 29 -> 0.00001
end...
Accuracy : 0.917
Classification report:
              precision    recall  f1-score   support

          ia       0.92      0.91      0.92       500
      nature       0.91      0.92      0.92       500

    accuracy                           0.92      1000
   macro avg       0.92      0.92      0.92      1000
weighted avg       0.92      0.92      0.92      1000

-----------------------------------------------------------------------------------------------------------
