wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: manurslf (manurslf301) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250319_203416-b64md65a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 0 (20250319-203415)
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/b64md65a
Operation on cuda
Using midjourney DATASET, Classes in dataset: ['ai', 'nature']
Additional tokens : 0
Training...
Loss epoch 0 -> 0.48286
Loss epoch 1 -> 0.12292
Loss epoch 2 -> 0.02986
Loss epoch 3 -> 0.03493
Loss epoch 4 -> 0.01923
Loss epoch 5 -> 0.01106
Loss epoch 6 -> 0.00989
Loss epoch 7 -> 0.01289
Loss epoch 8 -> 0.02351
Loss epoch 9 -> 0.01833
Loss epoch 10 -> 0.01199
Loss epoch 11 -> 0.01036
Loss epoch 12 -> 0.00897
Loss epoch 13 -> 0.02337
Loss epoch 14 -> 0.02111
Loss epoch 15 -> 0.00763
Loss epoch 16 -> 0.00582
Loss epoch 17 -> 0.00223
Loss epoch 18 -> 0.00634
Loss epoch 19 -> 0.00959
Loss epoch 20 -> 0.00240
Loss epoch 21 -> 0.00031
Loss epoch 22 -> 0.00006
Loss epoch 23 -> 0.00005
Loss epoch 24 -> 0.00004
Loss epoch 25 -> 0.00003
Loss epoch 26 -> 0.00003
Loss epoch 27 -> 0.00003
Loss epoch 28 -> 0.00003
Loss epoch 29 -> 0.00002
Loss epoch 30 -> 0.00002
Loss epoch 31 -> 0.00002
Loss epoch 32 -> 0.00002
Loss epoch 33 -> 0.00002
Loss epoch 34 -> 0.00002
Loss epoch 35 -> 0.00002
Loss epoch 36 -> 0.00001
Loss epoch 37 -> 0.00001
Loss epoch 38 -> 0.00001
Loss epoch 39 -> 0.00001
Loss epoch 40 -> 0.00001
Loss epoch 41 -> 0.00001
Loss epoch 42 -> 0.00001
Loss epoch 43 -> 0.00001
Loss epoch 44 -> 0.00001
Loss epoch 45 -> 0.00001
Loss epoch 46 -> 0.00001
Loss epoch 47 -> 0.00001
Loss epoch 48 -> 0.00001
Loss epoch 49 -> 0.00001
Loss epoch 50 -> 0.00001
Loss epoch 51 -> 0.00001
Loss epoch 52 -> 0.00001
Loss epoch 53 -> 0.00001
Loss epoch 54 -> 0.00001
Loss epoch 55 -> 0.00001
Loss epoch 56 -> 0.00001
Loss epoch 57 -> 0.00001
Loss epoch 58 -> 0.00001
Loss epoch 59 -> 0.00001
Loss epoch 60 -> 0.00001
Loss epoch 61 -> 0.00001
Loss epoch 62 -> 0.00001
Loss epoch 63 -> 0.00001
Loss epoch 64 -> 0.00001
Loss epoch 65 -> 0.00001
Loss epoch 66 -> 0.00001
Loss epoch 67 -> 0.00001
Loss epoch 68 -> 0.00001
Loss epoch 69 -> 0.00001
Loss epoch 70 -> 0.00001
Loss epoch 71 -> 0.00001
Loss epoch 72 -> 0.00001
Loss epoch 73 -> 0.00001
Loss epoch 74 -> 0.00001
Loss epoch 75 -> 0.00001
Loss epoch 76 -> 0.00001
Loss epoch 77 -> 0.00001
Loss epoch 78 -> 0.00001
Loss epoch 79 -> 0.00001
Loss epoch 80 -> 0.00001
Loss epoch 81 -> 0.00001
Loss epoch 82 -> 0.00001
Loss epoch 83 -> 0.00001
Loss epoch 84 -> 0.00001
Loss epoch 85 -> 0.00001
Loss epoch 86 -> 0.00001
Loss epoch 87 -> 0.00001
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00001
Loss epoch 90 -> 0.00001
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–‚â–ƒâ–â–…â–†â–ƒâ–ƒâ–ˆâ–ƒâ–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.887
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: ðŸš€ View run DS midjourney dataset - AT 0 (20250319-203415) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/b64md65a
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250319_203416-b64md65a/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250319_222152-b82c92hu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 10 (20250319-203415)
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/b82c92hu
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.887
Classification report:
              precision    recall  f1-score   support

          ia       0.89      0.88      0.89       500
      nature       0.88      0.89      0.89       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 10
Training...
Loss epoch 0 -> 1.01888
Loss epoch 1 -> 0.28313
Loss epoch 2 -> 0.09748
Loss epoch 3 -> 0.02505
Loss epoch 4 -> 0.01105
Loss epoch 5 -> 0.01190
Loss epoch 6 -> 0.00757
Loss epoch 7 -> 0.00052
Loss epoch 8 -> 0.00007
Loss epoch 9 -> 0.00004
Loss epoch 10 -> 0.00003
Loss epoch 11 -> 0.00002
Loss epoch 12 -> 0.00002
Loss epoch 13 -> 0.00002
Loss epoch 14 -> 0.00001
Loss epoch 15 -> 0.00001
Loss epoch 16 -> 0.00001
Loss epoch 17 -> 0.00001
Loss epoch 18 -> 0.00001
Loss epoch 19 -> 0.00001
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00000
Loss epoch 28 -> 0.00000
Loss epoch 29 -> 0.00000
Loss epoch 30 -> 0.00000
Loss epoch 31 -> 0.00000
Loss epoch 32 -> 0.00000
Loss epoch 33 -> 0.00000
Loss epoch 34 -> 0.00000
Loss epoch 35 -> 0.00000
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb: uploading history steps 99-99, summary, console lines 101-102
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:    Train/Loss â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.901
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: ðŸš€ View run DS midjourney dataset - AT 10 (20250319-203415) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/b82c92hu
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250319_222152-b82c92hu/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250320_001022-gsno8g67
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 25 (20250319-203415)
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/gsno8g67
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.901
Classification report:
              precision    recall  f1-score   support

          ia       0.92      0.88      0.90       500
      nature       0.89      0.92      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 25
Training...
Loss epoch 0 -> 1.19636
Loss epoch 1 -> 0.35614
Loss epoch 2 -> 0.14366
Loss epoch 3 -> 0.02574
Loss epoch 4 -> 0.01234
Loss epoch 5 -> 0.02827
Loss epoch 6 -> 0.00512
Loss epoch 7 -> 0.00206
Loss epoch 8 -> 0.00441
Loss epoch 9 -> 0.00703
Loss epoch 10 -> 0.01514
Loss epoch 11 -> 0.01056
Loss epoch 12 -> 0.00589
Loss epoch 13 -> 0.00316
Loss epoch 14 -> 0.00025
Loss epoch 15 -> 0.00006
Loss epoch 16 -> 0.00003
Loss epoch 17 -> 0.00002
Loss epoch 18 -> 0.00002
Loss epoch 19 -> 0.00002
Loss epoch 20 -> 0.00001
Loss epoch 21 -> 0.00001
Loss epoch 22 -> 0.00001
Loss epoch 23 -> 0.00001
Loss epoch 24 -> 0.00001
Loss epoch 25 -> 0.00001
Loss epoch 26 -> 0.00001
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00000
Loss epoch 33 -> 0.00000
Loss epoch 34 -> 0.00000
Loss epoch 35 -> 0.00000
Loss epoch 36 -> 0.00000
Loss epoch 37 -> 0.00000
Loss epoch 38 -> 0.00000
Loss epoch 39 -> 0.00000
Loss epoch 40 -> 0.00000
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    Train/Loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.902
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: ðŸš€ View run DS midjourney dataset - AT 25 (20250319-203415) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/gsno8g67
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250320_001022-gsno8g67/logs
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/users/r/rasolof2/BCs_emmanuelrasolofo/BCs_EmmanuelRasolofo/wandb/run-20250320_020001-k0clgo0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DS midjourney dataset - AT 50 (20250319-203415)
wandb: â­ï¸ View project at https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: ðŸš€ View run at https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/k0clgo0l
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.902
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.90      0.90       500
      nature       0.90      0.90      0.90       500

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000

-----------------------------------------------------------------------------------------------------------
Additional tokens : 50
Training...
Loss epoch 0 -> 0.95312
Loss epoch 1 -> 0.26417
Loss epoch 2 -> 0.08543
Loss epoch 3 -> 0.02766
Loss epoch 4 -> 0.00913
Loss epoch 5 -> 0.00722
Loss epoch 6 -> 0.01903
Loss epoch 7 -> 0.02110
Loss epoch 8 -> 0.01301
Loss epoch 9 -> 0.00305
Loss epoch 10 -> 0.00157
Loss epoch 11 -> 0.00922
Loss epoch 12 -> 0.01977
Loss epoch 13 -> 0.01361
Loss epoch 14 -> 0.00260
Loss epoch 15 -> 0.00093
Loss epoch 16 -> 0.00077
Loss epoch 17 -> 0.00044
Loss epoch 18 -> 0.01485
Loss epoch 19 -> 0.01179
Loss epoch 20 -> 0.00280
Loss epoch 21 -> 0.00038
Loss epoch 22 -> 0.00005
Loss epoch 23 -> 0.00003
Loss epoch 24 -> 0.00002
Loss epoch 25 -> 0.00002
Loss epoch 26 -> 0.00002
Loss epoch 27 -> 0.00001
Loss epoch 28 -> 0.00001
Loss epoch 29 -> 0.00001
Loss epoch 30 -> 0.00001
Loss epoch 31 -> 0.00001
Loss epoch 32 -> 0.00001
Loss epoch 33 -> 0.00001
Loss epoch 34 -> 0.00001
Loss epoch 35 -> 0.00001
Loss epoch 36 -> 0.00001
Loss epoch 37 -> 0.00001
Loss epoch 38 -> 0.00001
Loss epoch 39 -> 0.00001
Loss epoch 40 -> 0.00001
Loss epoch 41 -> 0.00000
Loss epoch 42 -> 0.00000
Loss epoch 43 -> 0.00000
Loss epoch 44 -> 0.00000
Loss epoch 45 -> 0.00000
Loss epoch 46 -> 0.00000
Loss epoch 47 -> 0.00000
Loss epoch 48 -> 0.00000
Loss epoch 49 -> 0.00000
Loss epoch 50 -> 0.00000
Loss epoch 51 -> 0.00000
Loss epoch 52 -> 0.00000
Loss epoch 53 -> 0.00000
Loss epoch 54 -> 0.00000
Loss epoch 55 -> 0.00000
Loss epoch 56 -> 0.00000
Loss epoch 57 -> 0.00000
Loss epoch 58 -> 0.00000
Loss epoch 59 -> 0.00000
Loss epoch 60 -> 0.00000
Loss epoch 61 -> 0.00000
Loss epoch 62 -> 0.00000
Loss epoch 63 -> 0.00000
Loss epoch 64 -> 0.00000
Loss epoch 65 -> 0.00000
Loss epoch 66 -> 0.00000
Loss epoch 67 -> 0.00000
Loss epoch 68 -> 0.00000
Loss epoch 69 -> 0.00000
Loss epoch 70 -> 0.00000
Loss epoch 71 -> 0.00000
Loss epoch 72 -> 0.00000
Loss epoch 73 -> 0.00000
Loss epoch 74 -> 0.00000
Loss epoch 75 -> 0.00000
Loss epoch 76 -> 0.00000
Loss epoch 77 -> 0.00000
Loss epoch 78 -> 0.00000
Loss epoch 79 -> 0.00000
Loss epoch 80 -> 0.00000
Loss epoch 81 -> 0.00000
Loss epoch 82 -> 0.00000
Loss epoch 83 -> 0.00000
Loss epoch 84 -> 0.00000
Loss epoch 85 -> 0.00000
Loss epoch 86 -> 0.00000
Loss epoch 87 -> 0.00000
Loss epoch 88 -> 0.00000
Loss epoch 89 -> 0.00000
Loss epoch 90 -> 0.00000
Loss epoch 91 -> 0.00000
Loss epoch 92 -> 0.00000
Loss epoch 93 -> 0.00000
Loss epoch 94 -> 0.00000
Loss epoch 95 -> 0.00000
Loss epoch 96 -> 0.00000
Loss epoch 97 -> 0.00000
Loss epoch 98 -> 0.00000
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: Test/Accuracy â–â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:    Train/Loss â–ˆâ–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb: Test/Accuracy 0.894
wandb:    Train/Loss 0.0
wandb:         epoch 99
wandb: 
wandb: ðŸš€ View run DS midjourney dataset - AT 50 (20250319-203415) at: https://wandb.ai/manurslf301/Encoder-DecoderProject/runs/k0clgo0l
wandb: â­ï¸ View project at: https://wandb.ai/manurslf301/Encoder-DecoderProject
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250320_020001-k0clgo0l/logs
Loss epoch 99 -> 0.00000
end...
Accuracy : 0.894
Classification report:
              precision    recall  f1-score   support

          ia       0.90      0.89      0.89       500
      nature       0.89      0.90      0.89       500

    accuracy                           0.89      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.89      0.89      0.89      1000

-----------------------------------------------------------------------------------------------------------
